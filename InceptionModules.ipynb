{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and preprocessing and miscellaneous functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use mnist_train.csv as our training data and mnist_test.csv as our validation set\n",
    "http://pjreddie.com/projects/mnist-in-csv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('mnist_train.csv',header=None)\n",
    "test_set = pd.read_csv('mnist_test.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9   ...   775  776  777  778  \\\n",
       "0    7    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "1    2    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "2    1    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "3    0    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "4    4    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "\n",
       "   779  780  781  782  783  784  \n",
       "0    0    0    0    0    0    0  \n",
       "1    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    0  \n",
       "3    0    0    0    0    0    0  \n",
       "4    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get labels in own array\n",
    "train_lb=np.array(train_set[0])\n",
    "test_lb=np.array(test_set[0])\n",
    "\n",
    "#one hot encode the labels\n",
    "train_lb=(np.arange(10) == train_lb[:,None]).astype(np.float32)\n",
    "test_lb=(np.arange(10) == test_lb[:,None]).astype(np.float32)\n",
    "\n",
    "#drop the labels column from training dataframe\n",
    "trainX=train_set.drop(0,axis=1)\n",
    "testX=test_set.drop(0,axis=1)\n",
    "\n",
    "#put in correct float32 array format\n",
    "trainX=np.array(trainX).astype(np.float32)\n",
    "testX=np.array(testX).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reformat the data so it's not flat\n",
    "trainX=trainX.reshape(len(trainX),28,28,1)\n",
    "testX = testX.reshape(len(testX),28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a validation set and remove it from the train set\n",
    "trainX,valX,train_lb,val_lb=trainX[0:(len(trainX)-500),:,:,:],trainX[(len(trainX)-500):len(trainX),:,:,:],\\\n",
    "                            train_lb[0:(len(trainX)-500),:],train_lb[(len(trainX)-500):len(trainX),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12137acc0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12043b828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#make sure the images are alright\n",
    "plt.imshow(trainX.reshape(len(trainX),28,28)[0],cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to batch the test data because running low on memory\n",
    "class test_batchs:\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        self.batch_index = 0\n",
    "    def nextBatch(self,batch_size):\n",
    "        if (batch_size+self.batch_index) > self.data.shape[0]:\n",
    "            print (\"batch sized is messed up\")\n",
    "        batch = self.data[self.batch_index:(self.batch_index+batch_size),:,:,:]\n",
    "        self.batch_index= self.batch_index+batch_size\n",
    "        return batch\n",
    "\n",
    "#set the test batchsize\n",
    "test_batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns accuracy of model\n",
    "def accuracy(target,predictions):\n",
    "    return(100.0*np.sum(np.argmax(target,1) == np.argmax(predictions,1))/target.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use os to get our current working directory so we can save variable\n",
    "file_path = os.getcwd()+'/model.ckpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Inception modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "map1 = 32\n",
    "map2 = 64\n",
    "num_fc1 = 700 #1028\n",
    "num_fc2 = 10\n",
    "reduce1x1 = 16\n",
    "dropout=0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #train data and labels\n",
    "    X = tf.placeholder(tf.float32,shape=(batch_size,28,28,1))\n",
    "    y_ = tf.placeholder(tf.float32,shape=(batch_size,10))\n",
    "    \n",
    "    #validation data\n",
    "    tf_valX = tf.placeholder(tf.float32,shape=(len(valX),28,28,1))\n",
    "    \n",
    "    #test data\n",
    "    tf_testX=tf.placeholder(tf.float32,shape=(test_batch_size,28,28,1))\n",
    "    \n",
    "    def createWeight(size,Name):\n",
    "        return tf.Variable(tf.truncated_normal(size, stddev=0.1),\n",
    "                          name=Name)\n",
    "    \n",
    "    def createBias(size,Name):\n",
    "        return tf.Variable(tf.constant(0.1,shape=size),\n",
    "                          name=Name)\n",
    "    \n",
    "    def conv2d_s1(x,W):\n",
    "        return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "    \n",
    "    def max_pool_3x3_s1(x):\n",
    "        return tf.nn.max_pool(x,ksize=[1,3,3,1],\n",
    "                             strides=[1,1,1,1],padding='SAME')\n",
    "    \n",
    "    \n",
    "    #Inception Module1\n",
    "    #\n",
    "    #follows input\n",
    "    W_conv1_1x1_1 = createWeight([1,1,1,map1],'W_conv1_1x1_1')\n",
    "    b_conv1_1x1_1 = createWeight([map1],'b_conv1_1x1_1')\n",
    "    \n",
    "    #follows input\n",
    "    W_conv1_1x1_2 = createWeight([1,1,1,reduce1x1],'W_conv1_1x1_2')\n",
    "    b_conv1_1x1_2 = createWeight([reduce1x1],'b_conv1_1x1_2')\n",
    "    \n",
    "    #follows input\n",
    "    W_conv1_1x1_3 = createWeight([1,1,1,reduce1x1],'W_conv1_1x1_3')\n",
    "    b_conv1_1x1_3 = createWeight([reduce1x1],'b_conv1_1x1_3')\n",
    "    \n",
    "    #follows 1x1_2\n",
    "    W_conv1_3x3 = createWeight([3,3,reduce1x1,map1],'W_conv1_3x3')\n",
    "    b_conv1_3x3 = createWeight([map1],'b_conv1_3x3')\n",
    "    \n",
    "    #follows 1x1_3\n",
    "    W_conv1_5x5 = createWeight([5,5,reduce1x1,map1],'W_conv1_5x5')\n",
    "    b_conv1_5x5 = createBias([map1],'b_conv1_5x5')\n",
    "    \n",
    "    #follows max pooling\n",
    "    W_conv1_1x1_4= createWeight([1,1,1,map1],'W_conv1_1x1_4')\n",
    "    b_conv1_1x1_4= createWeight([map1],'b_conv1_1x1_4')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Inception Module2\n",
    "    #\n",
    "    #follows inception1\n",
    "    W_conv2_1x1_1 = createWeight([1,1,4*map1,map2],'W_conv2_1x1_1')\n",
    "    b_conv2_1x1_1 = createWeight([map2],'b_conv2_1x1_1')\n",
    "    \n",
    "    #follows inception1\n",
    "    W_conv2_1x1_2 = createWeight([1,1,4*map1,reduce1x1],'W_conv2_1x1_2')\n",
    "    b_conv2_1x1_2 = createWeight([reduce1x1],'b_conv2_1x1_2')\n",
    "    \n",
    "    #follows inception1\n",
    "    W_conv2_1x1_3 = createWeight([1,1,4*map1,reduce1x1],'W_conv2_1x1_3')\n",
    "    b_conv2_1x1_3 = createWeight([reduce1x1],'b_conv2_1x1_3')\n",
    "    \n",
    "    #follows 1x1_2\n",
    "    W_conv2_3x3 = createWeight([3,3,reduce1x1,map2],'W_conv2_3x3')\n",
    "    b_conv2_3x3 = createWeight([map2],'b_conv2_3x3')\n",
    "    \n",
    "    #follows 1x1_3\n",
    "    W_conv2_5x5 = createWeight([5,5,reduce1x1,map2],'W_conv2_5x5')\n",
    "    b_conv2_5x5 = createBias([map2],'b_conv2_5x5')\n",
    "    \n",
    "    #follows max pooling\n",
    "    W_conv2_1x1_4= createWeight([1,1,4*map1,map2],'W_conv2_1x1_4')\n",
    "    b_conv2_1x1_4= createWeight([map2],'b_conv2_1x1_4')\n",
    "    \n",
    "    \n",
    "\n",
    "    #Fully connected layers\n",
    "    #since padding is same, the feature map with there will be 4 28*28*map2\n",
    "    W_fc1 = createWeight([28*28*(4*map2),num_fc1],'W_fc1')\n",
    "    b_fc1 = createBias([num_fc1],'b_fc1')\n",
    "    \n",
    "    W_fc2 = createWeight([num_fc1,num_fc2],'W_fc2')\n",
    "    b_fc2 = createBias([num_fc2],'b_fc2')\n",
    "\n",
    "    def model(x,train=True):\n",
    "        #Inception Module 1\n",
    "        conv1_1x1_1 = conv2d_s1(x,W_conv1_1x1_1)+b_conv1_1x1_1\n",
    "        conv1_1x1_2 = tf.nn.relu(conv2d_s1(x,W_conv1_1x1_2)+b_conv1_1x1_2)\n",
    "        conv1_1x1_3 = tf.nn.relu(conv2d_s1(x,W_conv1_1x1_3)+b_conv1_1x1_3)\n",
    "        conv1_3x3 = conv2d_s1(conv1_1x1_2,W_conv1_3x3)+b_conv1_3x3\n",
    "        conv1_5x5 = conv2d_s1(conv1_1x1_3,W_conv1_5x5)+b_conv1_5x5\n",
    "        maxpool1 = max_pool_3x3_s1(x)\n",
    "        conv1_1x1_4 = conv2d_s1(maxpool1,W_conv1_1x1_4)+b_conv1_1x1_4\n",
    "        \n",
    "        #concatenate all the feature maps and hit them with a relu\n",
    "        inception1 = tf.nn.relu(tf.concat([conv1_1x1_1,conv1_3x3,conv1_5x5,conv1_1x1_4],3))\n",
    "\n",
    "        \n",
    "        #Inception Module 2\n",
    "        conv2_1x1_1 = conv2d_s1(inception1,W_conv2_1x1_1)+b_conv2_1x1_1\n",
    "        conv2_1x1_2 = tf.nn.relu(conv2d_s1(inception1,W_conv2_1x1_2)+b_conv2_1x1_2)\n",
    "        conv2_1x1_3 = tf.nn.relu(conv2d_s1(inception1,W_conv2_1x1_3)+b_conv2_1x1_3)\n",
    "        conv2_3x3 = conv2d_s1(conv2_1x1_2,W_conv2_3x3)+b_conv2_3x3\n",
    "        conv2_5x5 = conv2d_s1(conv2_1x1_3,W_conv2_5x5)+b_conv2_5x5\n",
    "        maxpool2 = max_pool_3x3_s1(inception1)\n",
    "        conv2_1x1_4 = conv2d_s1(maxpool2,W_conv2_1x1_4)+b_conv2_1x1_4\n",
    "        \n",
    "        #concatenate all the feature maps and hit them with a relu\n",
    "        inception2 = tf.nn.relu(tf.concat([conv2_1x1_1,conv2_3x3,conv2_5x5,conv2_1x1_4],3))\n",
    "\n",
    "        #flatten features for fully connected layer\n",
    "        inception2_flat = tf.reshape(inception2,[-1,28*28*4*map2])\n",
    "        \n",
    "        #Fully connected layers\n",
    "        if train:\n",
    "            h_fc1 =tf.nn.dropout(tf.nn.relu(tf.matmul(inception2_flat,W_fc1)+b_fc1),dropout)\n",
    "        else:\n",
    "            h_fc1 = tf.nn.relu(tf.matmul(inception2_flat,W_fc1)+b_fc1)\n",
    "\n",
    "        return tf.matmul(h_fc1,W_fc2)+b_fc2\n",
    "    \n",
    "#     tf.nn.softmax_cross_entropy_with_logits(logits = yPredbyNN, labels=Y)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=model(X),labels=y_))\n",
    "    opt = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "    \n",
    "    predictions_val = tf.nn.softmax(model(tf_valX,train=False))\n",
    "    predictions_test = tf.nn.softmax(model(tf_testX,train=False))\n",
    "    \n",
    "    #initialize variable\n",
    "    init = tf.initialize_all_variables()\n",
    "    \n",
    "    #use to save variables so we can pick up later\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized.\n",
      "step: 0\n",
      "validation accuracy: 20.0\n",
      " \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-2fe0960bc89c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_lb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtf_valX\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mvalX\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 20000\n",
    "sess = tf.Session(graph=graph)\n",
    "\n",
    "#initialize variables\n",
    "sess.run(init)\n",
    "print(\"Model initialized.\")\n",
    "\n",
    "#set use_previous=1 to use file_path model\n",
    "#set use_previous=0 to start model from scratch\n",
    "use_previous = 0\n",
    "\n",
    "#use the previous model or don't and initialize variables\n",
    "if use_previous:\n",
    "    saver.restore(sess,file_path)\n",
    "    print(\"Model restored.\")\n",
    "\n",
    "#training\n",
    "for s in range(num_steps):\n",
    "    offset = (s*batch_size) % (len(trainX)-batch_size)\n",
    "    batch_x,batch_y = trainX[offset:(offset+batch_size),:],train_lb[offset:(offset+batch_size),:]\n",
    "    feed_dict={X : batch_x, y_ : batch_y}\n",
    "    _,loss_value = sess.run([opt,loss],feed_dict=feed_dict)\n",
    "    if s%100 == 0:\n",
    "        feed_dict = {tf_valX : valX}\n",
    "        preds=sess.run(predictions_val,feed_dict=feed_dict)\n",
    "        \n",
    "        print (\"step: \"+str(s))\n",
    "        print (\"validation accuracy: \"+str(accuracy(val_lb,preds)))\n",
    "        print (\" \")\n",
    "        \n",
    "    #get test accuracy and save model\n",
    "    if s == (num_steps-1):\n",
    "        #create an array to store the outputs for the test\n",
    "        result = np.array([]).reshape(0,10)\n",
    "\n",
    "        #use the batches class\n",
    "        batch_testX=test_batchs(testX)\n",
    "\n",
    "        for i in range(len(testX)/test_batch_size):\n",
    "            feed_dict = {tf_testX : batch_testX.nextBatch(test_batch_size)}\n",
    "            preds=sess.run(predictions_test, feed_dict=feed_dict)\n",
    "            result=np.concatenate((result,preds),axis=0)\n",
    "        \n",
    "        print (\"test accuracy: \"+str(accuracy(test_lb,result)))\n",
    "        \n",
    "        save_path = saver.save(sess,file_path)\n",
    "        print(\"Model saved.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
